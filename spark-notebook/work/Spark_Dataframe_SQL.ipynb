{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96027ea9-c511-4436-a66f-4a15f2fd08a2",
   "metadata": {},
   "source": [
    "# Spark Dataframe and Spark SQL\n",
    "\n",
    "* Demo between Spark Dataframe and Spark RDD\n",
    "* Spark Dataframe Operations\n",
    "* Spark SQL Operations\n",
    "* Save dataframe into MinIO\n",
    "    * Write a dataframe by Delta Lake format\n",
    "    * Write a dataframe by Parquet format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "6cecccaa-d5c9-422b-b933-928390863363",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import count, max\n",
    "from pyspark.sql.functions import asc, desc, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "caed7a48-9e6d-48b1-94cd-af416f419fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName('Ingest checkin table into bronze') \\\n",
    "    .master('spark://spark-master:7077') \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "    .config(\"hive.metastore.uris\", \"thrift://hive-metastore:9083\") \\\n",
    "    .config('spark.hadoop.fs.s3a.aws.credentials.provider', 'org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider') \\\n",
    "    .config('spark.sql.warehouse.dir', f's3a://lakehouse/') \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "863b0b47-c72b-4e21-b4ed-69bfc180664d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://f96e811bbabb:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://spark-master:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Ingest checkin table into bronze</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fa97b783100>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f2b192-281a-4897-934d-49d25b473087",
   "metadata": {},
   "source": [
    "## Demo between Spark Dataframe and Spark RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5d280a-81cf-4576-b4b6-94a9d7319f3a",
   "metadata": {},
   "source": [
    "### 1. Spark RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9d81d0f6-3d3e-4446-aeae-ceacd7898fe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Apache Spark has its architectural foundation in the resilient distributed dataset (RDD), a read-only multiset of data items distributed over a cluster of machines, that is maintained in a fault-tolerant way. The Dataframe API was released as an abstraction on top of the RDD, followed by the Dataset API. In Spark 1.x, the RDD was the primary application programming interface (API), but as of Spark 2.x use of the Dataset API is encouraged even though the RDD API is not deprecated. The RDD technology still underlies the Dataset API.',\n",
       " '',\n",
       " \"Spark and its RDDs were developed in 2012 in response to limitations in the MapReduce cluster computing paradigm, which forces a particular linear dataflow structure on distributed programs: MapReduce programs read input data from disk, map a function across the data, reduce the results of the map, and store reduction results on disk. Spark's RDDs function as a working set for distributed programs that offers a (deliberately) restricted form of distributed shared memory.\"]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_rdd = spark.sparkContext.textFile(\"s3a://lakehouse/test_csv/data.txt\")\n",
    "text_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b79a561d-e62d-43fd-b06c-8fdf11379c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working: 1\n",
      "which: 1\n",
      "were: 1\n",
      "way.: 1\n",
      "was: 2\n",
      "use: 1\n",
      "underlies: 1\n",
      "top: 1\n",
      "to: 1\n",
      "though: 1\n",
      "the: 12\n",
      "that: 2\n",
      "technology: 1\n",
      "structure: 1\n",
      "store: 1\n",
      "still: 1\n",
      "shared: 1\n",
      "set: 1\n",
      "results: 2\n",
      "restricted: 1\n"
     ]
    }
   ],
   "source": [
    "seqFunc = (lambda x, y: x+y)\n",
    "combFunc = (lambda c1, c2: c1+c2)\n",
    "\n",
    "wordcount_rdd = text_rdd.flatMap(lambda line: line.split(' '))\\\n",
    "                        .map(lambda word: (word, 1))\\\n",
    "                        .aggregateByKey(0, seqFunc, combFunc)\\\n",
    "                        .sortByKey(False)\n",
    "\n",
    "for word in wordcount_rdd.take(20):\n",
    "    print(f\"{word[0]}: {word[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a505fba5-f3cf-42e6-b4b8-c462bf99624c",
   "metadata": {},
   "source": [
    "### 2. Spark Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "094d4734-756e-414e-888b-7045891921d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                text|\n",
      "+--------------------+\n",
      "|Apache Spark has ...|\n",
      "|                    |\n",
      "|Spark and its RDD...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_df = spark.read.text(\"s3a://lakehouse/test_csv/data.txt\")\n",
    "columns = ['text']\n",
    "text_df = text_df.toDF(*columns)\n",
    "text_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5d8440fc-1e1d-4abf-86d1-b0fc7e793bf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(word='working', count=1),\n",
       " Row(word='which', count=1),\n",
       " Row(word='were', count=1),\n",
       " Row(word='way.', count=1),\n",
       " Row(word='was', count=2),\n",
       " Row(word='use', count=1),\n",
       " Row(word='underlies', count=1),\n",
       " Row(word='top', count=1),\n",
       " Row(word='to', count=1),\n",
       " Row(word='though', count=1),\n",
       " Row(word='the', count=12),\n",
       " Row(word='that', count=2),\n",
       " Row(word='technology', count=1),\n",
       " Row(word='structure', count=1),\n",
       " Row(word='store', count=1),\n",
       " Row(word='still', count=1),\n",
       " Row(word='shared', count=1),\n",
       " Row(word='set', count=1),\n",
       " Row(word='results', count=2),\n",
       " Row(word='restricted', count=1)]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordcount_df = text_df.selectExpr(\"explode(split(text, ' ')) as word\")\\\n",
    "                    .groupBy('word')\\\n",
    "                    .agg(count('word').alias('count'))\\\n",
    "                    .sort(desc('word'))\n",
    "wordcount_df.take(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eddb431-f622-4ac3-9657-2d61bec6d741",
   "metadata": {},
   "source": [
    "## Spark Dataframe Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370abeb3-bcf9-4778-81b5-936df62a4583",
   "metadata": {},
   "source": [
    "### 1. Loading data from MinIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "673304ff-992c-4e65-88f3-4bc50332351a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows:  599\n",
      "+-----------+--------+----------+---------+--------------------+----------+----------+-------------------+-------------------+------+\n",
      "|customer_id|store_id|first_name|last_name|               email|address_id|activebool|        create_date|        last_update|active|\n",
      "+-----------+--------+----------+---------+--------------------+----------+----------+-------------------+-------------------+------+\n",
      "|          1|       1|      MARY|    SMITH|MARY.SMITH@sakila...|         5|         t|2006-02-14 00:00:00|2006-02-15 09:57:20|     1|\n",
      "|          2|       1|  PATRICIA|  JOHNSON|PATRICIA.JOHNSON@...|         6|         t|2006-02-14 00:00:00|2006-02-15 09:57:20|     1|\n",
      "|          3|       1|     LINDA| WILLIAMS|LINDA.WILLIAMS@sa...|         7|         t|2006-02-14 00:00:00|2006-02-15 09:57:20|     1|\n",
      "|          4|       2|   BARBARA|    JONES|BARBARA.JONES@sak...|         8|         t|2006-02-14 00:00:00|2006-02-15 09:57:20|     1|\n",
      "|          5|       1| ELIZABETH|    BROWN|ELIZABETH.BROWN@s...|         9|         t|2006-02-14 00:00:00|2006-02-15 09:57:20|     1|\n",
      "|          6|       2|  JENNIFER|    DAVIS|JENNIFER.DAVIS@sa...|        10|         t|2006-02-14 00:00:00|2006-02-15 09:57:20|     1|\n",
      "|          7|       1|     MARIA|   MILLER|MARIA.MILLER@saki...|        11|         t|2006-02-14 00:00:00|2006-02-15 09:57:20|     1|\n",
      "|          8|       2|     SUSAN|   WILSON|SUSAN.WILSON@saki...|        12|         t|2006-02-14 00:00:00|2006-02-15 09:57:20|     1|\n",
      "|          9|       2|  MARGARET|    MOORE|MARGARET.MOORE@sa...|        13|         t|2006-02-14 00:00:00|2006-02-15 09:57:20|     1|\n",
      "|         10|       1|   DOROTHY|   TAYLOR|DOROTHY.TAYLOR@sa...|        14|         t|2006-02-14 00:00:00|2006-02-15 09:57:20|     1|\n",
      "|         11|       2|      LISA| ANDERSON|LISA.ANDERSON@sak...|        15|         t|2006-02-14 00:00:00|2006-02-15 09:57:20|     1|\n",
      "|         12|       1|     NANCY|   THOMAS|NANCY.THOMAS@saki...|        16|         t|2006-02-14 00:00:00|2006-02-15 09:57:20|     1|\n",
      "|         13|       2|     KAREN|  JACKSON|KAREN.JACKSON@sak...|        17|         t|2006-02-14 00:00:00|2006-02-15 09:57:20|     1|\n",
      "|         14|       2|     BETTY|    WHITE|BETTY.WHITE@sakil...|        18|         t|2006-02-14 00:00:00|2006-02-15 09:57:20|     1|\n",
      "|         15|       1|     HELEN|   HARRIS|HELEN.HARRIS@saki...|        19|         t|2006-02-14 00:00:00|2006-02-15 09:57:20|     1|\n",
      "|         16|       2|    SANDRA|   MARTIN|SANDRA.MARTIN@sak...|        20|         t|2006-02-14 00:00:00|2006-02-15 09:57:20|     0|\n",
      "|         17|       1|     DONNA| THOMPSON|DONNA.THOMPSON@sa...|        21|         t|2006-02-14 00:00:00|2006-02-15 09:57:20|     1|\n",
      "|         18|       2|     CAROL|   GARCIA|CAROL.GARCIA@saki...|        22|         t|2006-02-14 00:00:00|2006-02-15 09:57:20|     1|\n",
      "|         19|       1|      RUTH| MARTINEZ|RUTH.MARTINEZ@sak...|        23|         t|2006-02-14 00:00:00|2006-02-15 09:57:20|     1|\n",
      "|         20|       2|    SHARON| ROBINSON|SHARON.ROBINSON@s...|        24|         t|2006-02-14 00:00:00|2006-02-15 09:57:20|     1|\n",
      "+-----------+--------+----------+---------+--------------------+----------+----------+-------------------+-------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_df = spark.read.csv(\"s3a://lakehouse/test_csv/customer.csv\", header=True, inferSchema=True, samplingRatio=0.1)\n",
    "print(\"Number of rows: \", customer_df.count())\n",
    "customer_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6ca4e70f-2b5a-46fa-9aac-cccafa840506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- store_id: integer (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- address_id: integer (nullable = true)\n",
      " |-- activebool: string (nullable = true)\n",
      " |-- create_date: timestamp (nullable = true)\n",
      " |-- last_update: timestamp (nullable = true)\n",
      " |-- active: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44eb8f0b-2b72-4826-a041-66880c63d757",
   "metadata": {},
   "source": [
    "### 2. Selecting columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e76f5ca1-4483-4269-88d1-199739303a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+----------+\n",
      "|customer_id|               email|address_id|\n",
      "+-----------+--------------------+----------+\n",
      "|          1|MARY.SMITH@sakila...|         5|\n",
      "|          2|PATRICIA.JOHNSON@...|         6|\n",
      "|          3|LINDA.WILLIAMS@sa...|         7|\n",
      "|          4|BARBARA.JONES@sak...|         8|\n",
      "|          5|ELIZABETH.BROWN@s...|         9|\n",
      "|          6|JENNIFER.DAVIS@sa...|        10|\n",
      "|          7|MARIA.MILLER@saki...|        11|\n",
      "|          8|SUSAN.WILSON@saki...|        12|\n",
      "|          9|MARGARET.MOORE@sa...|        13|\n",
      "|         10|DOROTHY.TAYLOR@sa...|        14|\n",
      "|         11|LISA.ANDERSON@sak...|        15|\n",
      "|         12|NANCY.THOMAS@saki...|        16|\n",
      "|         13|KAREN.JACKSON@sak...|        17|\n",
      "|         14|BETTY.WHITE@sakil...|        18|\n",
      "|         15|HELEN.HARRIS@saki...|        19|\n",
      "|         16|SANDRA.MARTIN@sak...|        20|\n",
      "|         17|DONNA.THOMPSON@sa...|        21|\n",
      "|         18|CAROL.GARCIA@saki...|        22|\n",
      "|         19|RUTH.MARTINEZ@sak...|        23|\n",
      "|         20|SHARON.ROBINSON@s...|        24|\n",
      "+-----------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## TODO: selecting columns by select()\n",
    "temp_df = customer_df.select(\"customer_id\", \"email\", \"address_id\")\n",
    "temp_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b59376f-ac38-4660-9daf-fc6ace9dfa01",
   "metadata": {},
   "source": [
    "### 3. Filtering records following conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "8f943fc3-9792-40c2-88a9-e229c3001e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows:  584\n",
      "+-----------+--------+----------+---------+--------------------+----------+----------+-------------------+-------------------+------+\n",
      "|customer_id|store_id|first_name|last_name|               email|address_id|activebool|        create_date|        last_update|active|\n",
      "+-----------+--------+----------+---------+--------------------+----------+----------+-------------------+-------------------+------+\n",
      "|          1|       1|      MARY|    SMITH|MARY.SMITH@sakila...|         5|         t|2006-02-14 00:00:00|2006-02-15 09:57:20|     1|\n",
      "|          2|       1|  PATRICIA|  JOHNSON|PATRICIA.JOHNSON@...|         6|         t|2006-02-14 00:00:00|2006-02-15 09:57:20|     1|\n",
      "|          3|       1|     LINDA| WILLIAMS|LINDA.WILLIAMS@sa...|         7|         t|2006-02-14 00:00:00|2006-02-15 09:57:20|     1|\n",
      "|          4|       2|   BARBARA|    JONES|BARBARA.JONES@sak...|         8|         t|2006-02-14 00:00:00|2006-02-15 09:57:20|     1|\n",
      "|          5|       1| ELIZABETH|    BROWN|ELIZABETH.BROWN@s...|         9|         t|2006-02-14 00:00:00|2006-02-15 09:57:20|     1|\n",
      "|          6|       2|  JENNIFER|    DAVIS|JENNIFER.DAVIS@sa...|        10|         t|2006-02-14 00:00:00|2006-02-15 09:57:20|     1|\n",
      "|          7|       1|     MARIA|   MILLER|MARIA.MILLER@saki...|        11|         t|2006-02-14 00:00:00|2006-02-15 09:57:20|     1|\n",
      "|          8|       2|     SUSAN|   WILSON|SUSAN.WILSON@saki...|        12|         t|2006-02-14 00:00:00|2006-02-15 09:57:20|     1|\n",
      "|          9|       2|  MARGARET|    MOORE|MARGARET.MOORE@sa...|        13|         t|2006-02-14 00:00:00|2006-02-15 09:57:20|     1|\n",
      "|         10|       1|   DOROTHY|   TAYLOR|DOROTHY.TAYLOR@sa...|        14|         t|2006-02-14 00:00:00|2006-02-15 09:57:20|     1|\n",
      "|         11|       2|      LISA| ANDERSON|LISA.ANDERSON@sak...|        15|         t|2006-02-14 00:00:00|2006-02-15 09:57:20|     1|\n",
      "|         12|       1|     NANCY|   THOMAS|NANCY.THOMAS@saki...|        16|         t|2006-02-14 00:00:00|2006-02-15 09:57:20|     1|\n",
      "|         13|       2|     KAREN|  JACKSON|KAREN.JACKSON@sak...|        17|         t|2006-02-14 00:00:00|2006-02-15 09:57:20|     1|\n",
      "|         14|       2|     BETTY|    WHITE|BETTY.WHITE@sakil...|        18|         t|2006-02-14 00:00:00|2006-02-15 09:57:20|     1|\n",
      "|         15|       1|     HELEN|   HARRIS|HELEN.HARRIS@saki...|        19|         t|2006-02-14 00:00:00|2006-02-15 09:57:20|     1|\n",
      "|         17|       1|     DONNA| THOMPSON|DONNA.THOMPSON@sa...|        21|         t|2006-02-14 00:00:00|2006-02-15 09:57:20|     1|\n",
      "|         18|       2|     CAROL|   GARCIA|CAROL.GARCIA@saki...|        22|         t|2006-02-14 00:00:00|2006-02-15 09:57:20|     1|\n",
      "|         19|       1|      RUTH| MARTINEZ|RUTH.MARTINEZ@sak...|        23|         t|2006-02-14 00:00:00|2006-02-15 09:57:20|     1|\n",
      "|         20|       2|    SHARON| ROBINSON|SHARON.ROBINSON@s...|        24|         t|2006-02-14 00:00:00|2006-02-15 09:57:20|     1|\n",
      "|         21|       1|  MICHELLE|    CLARK|MICHELLE.CLARK@sa...|        25|         t|2006-02-14 00:00:00|2006-02-15 09:57:20|     1|\n",
      "+-----------+--------+----------+---------+--------------------+----------+----------+-------------------+-------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## TODO: filtering records by filter()\n",
    "temp_df = customer_df.filter(customer_df.active==1).orderBy(col(\"address_id\").asc())\n",
    "print(\"Number of rows: \", temp_df.count())\n",
    "temp_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29073bbc-1320-492a-ab3f-3414e64c8dbb",
   "metadata": {},
   "source": [
    "### 4. Grouping records with aggregation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "28e35b74-0414-411b-8f2e-318edc3403a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+-----------------------------+\n",
      "|store_id|quantity|recent_activity_customer_date|\n",
      "+--------+--------+-----------------------------+\n",
      "|       2|     273|          2006-02-15 09:57:20|\n",
      "|       1|     326|          2006-02-15 09:57:20|\n",
      "+--------+--------+-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## TODO: Grouping records by using groupBy()\n",
    "temp_df = customer_df.groupBy(\"store_id\")\\\n",
    "                    .agg(count(\"customer_id\").alias(\"quantity\"), max(\"last_update\").alias(\"recent_activity_customer_date\"))\n",
    "temp_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464b0efb-c70a-45bc-aab0-93dfd1272379",
   "metadata": {},
   "source": [
    "### 5. Joining the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a56b746-7119-4526-89e0-7d31f5f8e0bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows:  36938\n",
      "+---------+-------------------+------------+-----------+-------------------+--------+-------------------+\n",
      "|rental_id|        rental_date|inventory_id|customer_id|        return_date|staff_id|        last_update|\n",
      "+---------+-------------------+------------+-----------+-------------------+--------+-------------------+\n",
      "|   281475|2010-01-01 04:21:21|        1372|        238|2010-01-03 00:00:00|      68|2010-01-01 02:18:47|\n",
      "|   281476|2010-01-01 07:47:23|        3430|        167|2010-01-03 04:21:21|     122|2010-01-01 04:05:03|\n",
      "|   281477|2010-01-01 09:15:59|        1161|        446|2010-01-05 07:47:23|       1|2010-01-01 05:08:26|\n",
      "|   281478|2010-01-01 15:20:16|         743|        565|2010-01-06 09:15:59|     120|2010-01-01 06:03:18|\n",
      "|   281479|2010-01-01 20:41:05|        1919|        573|2010-01-06 15:20:16|      21|2010-01-01 06:20:04|\n",
      "|   281480|2010-01-01 23:45:30|        3323|        480|2010-01-05 20:41:05|      72|2010-01-01 06:43:21|\n",
      "|   281481|2010-01-02 04:39:49|         165|         75|2010-01-05 23:45:30|     124|2010-01-01 07:05:52|\n",
      "|   281482|2010-01-02 05:53:43|        4470|         52|2010-01-06 04:39:49|     123|2010-01-01 09:00:01|\n",
      "|   281483|2010-01-02 07:26:19|        3135|        591|2010-01-07 05:53:43|      75|2010-01-01 10:18:55|\n",
      "|   281484|2010-01-02 09:04:59|        4486|        462|2010-01-07 07:26:19|      74|2010-01-01 12:05:55|\n",
      "|   281485|2010-01-02 10:00:13|        1300|        142|2010-01-05 09:04:59|      20|2010-01-01 13:55:18|\n",
      "|   281486|2010-01-02 11:33:41|        2788|        429|2010-01-05 10:00:13|     114|2010-01-01 16:04:33|\n",
      "|   281487|2010-01-02 12:11:04|        1203|        468|2010-01-05 11:33:41|      68|2010-01-01 16:59:37|\n",
      "|   281488|2010-01-02 13:17:17|        1882|        114|2010-01-04 12:11:04|     124|2010-01-01 19:22:41|\n",
      "|   281489|2010-01-02 15:15:09|        2387|        472|2010-01-06 13:17:17|      13|2010-01-01 20:14:03|\n",
      "|   281490|2010-01-02 17:28:34|        1798|        579|2010-01-05 15:15:09|     114|2010-01-01 20:30:42|\n",
      "|   281491|2010-01-02 19:38:45|        2039|        357|2010-01-07 17:28:34|      21|2010-01-01 22:09:10|\n",
      "|   281492|2010-01-02 21:48:32|        2008|         18|2010-01-03 19:38:45|       7|2010-01-01 23:28:29|\n",
      "|   281493|2010-01-03 00:13:16|        2080|        529|2010-01-05 21:48:32|      76|2010-01-02 01:30:56|\n",
      "|   281494|2010-01-03 01:29:12|        1229|        312|2010-01-04 00:13:16|      71|2010-01-02 03:04:21|\n",
      "+---------+-------------------+------------+-----------+-------------------+--------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## TODO: Joining dataframe by join()\n",
    "rental_df = spark.read.csv(\"s3a://lakehouse/test_csv/rental.csv\", header=True, inferSchema=True)\n",
    "print(\"Number of rows: \", rental_df.count())\n",
    "rental_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "55eb3932-7120-4e00-b74d-fa50a5fae10e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(customer_id=238, store_id=1, email='NELLIE.GARRETT@sakilacustomer.org', rental_id=281475, rental_date=datetime.datetime(2010, 1, 1, 4, 21, 21), staff_id=68),\n",
       " Row(customer_id=167, store_id=2, email='SALLY.PIERCE@sakilacustomer.org', rental_id=281476, rental_date=datetime.datetime(2010, 1, 1, 7, 47, 23), staff_id=122),\n",
       " Row(customer_id=446, store_id=2, email='THEODORE.CULP@sakilacustomer.org', rental_id=281477, rental_date=datetime.datetime(2010, 1, 1, 9, 15, 59), staff_id=1),\n",
       " Row(customer_id=565, store_id=2, email='JAIME.NETTLES@sakilacustomer.org', rental_id=281478, rental_date=datetime.datetime(2010, 1, 1, 15, 20, 16), staff_id=120),\n",
       " Row(customer_id=573, store_id=1, email='BYRON.BOX@sakilacustomer.org', rental_id=281479, rental_date=datetime.datetime(2010, 1, 1, 20, 41, 5), staff_id=21)]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined_df = customer_df.join(other=rental_df, on=\"customer_id\", how=\"inner\")\\\n",
    "                    .select(\"customer_id\", \"store_id\", \"email\", \"rental_id\", \"rental_date\", \"staff_id\")\n",
    "joined_df.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63df6b57-4b5d-46d5-b8ec-45f546a40e95",
   "metadata": {},
   "source": [
    "### 6. Sorting records by columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "2767f537-e608-4c10-9595-5f76763abb16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(rental_id=318412, rental_date=datetime.datetime(2020, 12, 30, 0, 23, 16), inventory_id=2200, customer_id=44, return_date=datetime.datetime(2020, 12, 30, 23, 48, 2), staff_id=76, last_update=datetime.datetime(2015, 3, 31, 13, 16, 31)),\n",
       " Row(rental_id=318411, rental_date=datetime.datetime(2020, 12, 29, 23, 48, 2), inventory_id=2217, customer_id=65, return_date=datetime.datetime(2021, 1, 3, 22, 59, 50), staff_id=11, last_update=datetime.datetime(2015, 3, 31, 11, 32, 29)),\n",
       " Row(rental_id=318410, rental_date=datetime.datetime(2020, 12, 29, 22, 59, 50), inventory_id=1440, customer_id=235, return_date=datetime.datetime(2021, 1, 2, 22, 33, 28), staff_id=7, last_update=datetime.datetime(2015, 3, 31, 10, 0, 55)),\n",
       " Row(rental_id=318409, rental_date=datetime.datetime(2020, 12, 29, 22, 33, 28), inventory_id=2809, customer_id=346, return_date=datetime.datetime(2020, 12, 31, 22, 14, 3), staff_id=64, last_update=datetime.datetime(2015, 3, 31, 9, 13, 12)),\n",
       " Row(rental_id=318408, rental_date=datetime.datetime(2020, 12, 29, 22, 14, 3), inventory_id=3089, customer_id=554, return_date=datetime.datetime(2020, 12, 31, 21, 10, 26), staff_id=78, last_update=datetime.datetime(2015, 3, 31, 8, 50))]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_df = rental_df.orderBy(col(\"rental_date\").desc(), col(\"customer_id\").asc())\n",
    "sorted_df.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc401cb3-e7ed-42fe-b965-2ae28ed7fb51",
   "metadata": {},
   "source": [
    "### 7. Window Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe554ae-65b7-4b78-afc0-eec1bdac0f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Using Window Function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50fa757-8836-4ac0-92ea-c541f684a2e1",
   "metadata": {},
   "source": [
    "## Spark SQL Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f779e83d-4abc-4c43-afbf-08211a896dcf",
   "metadata": {},
   "source": [
    "### 1. Register dataframe as Temporary table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "28b73d0e-a101-4a1c-bd74-58ea9500e0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "payment_df = spark.read.csv(\"s3a://lakehouse/test_csv/payment.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "703408fa-759d-40f6-81ab-415da91937e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df.createOrReplaceTempView(\"vw_customer\")\n",
    "rental_df.createOrReplaceTempView(\"vw_rental\")\n",
    "payment_df.createOrReplaceTempView(\"vw_payment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "10da852f-33f6-4b94-aa23-207773d24861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|current_database()|\n",
      "+------------------+\n",
      "|           default|\n",
      "+------------------+\n",
      "\n",
      "+---------+-----------+-----------+\n",
      "|namespace|  tableName|isTemporary|\n",
      "+---------+-----------+-----------+\n",
      "|         |vw_customer|      false|\n",
      "|         | vw_payment|      false|\n",
      "|         |  vw_rental|      false|\n",
      "+---------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT current_database()\").show()\n",
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "1c26f9a6-9357-4f2d-9aab-6fa2ad02570a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in vw_customer 599\n",
      "Number of rows in vw_payment 33286\n",
      "Number of rows in vw_rental 36938\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of rows in vw_customer\", spark.sql(\"SELECT * FROM vw_customer\").count())\n",
    "print(\"Number of rows in vw_payment\", spark.sql(\"SELECT * FROM vw_payment\").count())\n",
    "print(\"Number of rows in vw_rental\", spark.sql(\"SELECT * FROM vw_rental\").count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8081bd63-ba23-4866-b71a-e3d04aae9d4a",
   "metadata": {},
   "source": [
    "### 2. Manipulate data with SQL script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a539f7-bdaa-4219-b580-3879a21209d4",
   "metadata": {},
   "source": [
    "- **Joining views and creating a new view**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "1c542581-6d2a-440c-9c8a-18a36764f812",
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlStr = \"\"\" \n",
    "    SELECT\n",
    "        c.customer_id, c.active, CONCAT(c.first_name,' ', c.last_name) AS full_name, r.rental_id, r.rental_date, p.amount, p.payment_date\n",
    "    FROM vw_customer c\n",
    "    INNER JOIN vw_rental r ON r.customer_id=c.customer_id\n",
    "    INNER JOIN vw_payment p ON p.rental_id=r.rental_id\n",
    "    ORDER BY c.customer_id, rental_date\n",
    "\"\"\"\n",
    "activities_df = spark.sql(sqlStr)\n",
    "activities_df.createOrReplaceTempView(\"vw_activity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "a314141f-d338-446e-b829-e788bcf81fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+----------+---------+-------------------+------+-------------------+\n",
      "|customer_id|active| full_name|rental_id|        rental_date|amount|       payment_date|\n",
      "+-----------+------+----------+---------+-------------------+------+-------------------+\n",
      "|          1|     1|MARY SMITH|   281651|2010-01-21 09:37:31|  53.4|2010-01-22 08:02:16|\n",
      "|          1|     1|MARY SMITH|   281990|2010-02-25 20:16:23|  60.3|2010-03-02 14:44:06|\n",
      "|          1|     1|MARY SMITH|   282094|2010-03-07 19:34:22|   7.6|2010-03-10 16:55:31|\n",
      "|          1|     1|MARY SMITH|   282140|2010-03-19 14:24:00|  44.2|2010-03-22 08:20:36|\n",
      "|          1|     1|MARY SMITH|   282774|2010-07-25 09:54:25|  49.3|2010-07-30 06:14:08|\n",
      "+-----------+------+----------+---------+-------------------+------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM vw_activity LIMIT 5\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f4fe49-9436-4bb7-a5a6-fedb6bdb8b4b",
   "metadata": {},
   "source": [
    "- **Window Function in Spark SQL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "3c45e775-fb3e-4187-8b6e-00bca90b144b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlStr = \"\"\"\n",
    "    SELECT DISTINCT\n",
    "        customer_id, full_name,\n",
    "        DATEDIFF(latest_activity_date, earliest_activity_date) AS recency,\n",
    "        AVG(amount) OVER (PARTITION BY customer_id) AS avg_amount\n",
    "    FROM (\n",
    "        SELECT\n",
    "            *,\n",
    "            MAX(rental_date) OVER (PARTITION BY customer_id) AS latest_activity_date,\n",
    "            MIN(rental_date) OVER (PARTITION BY customer_id) AS earliest_activity_date\n",
    "        FROM vw_activity ac\n",
    "    )\n",
    "\"\"\"\n",
    "recency_df = spark.sql(sqlStr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "4b3513e4-4caa-4f69-b038-a822d192a13a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+-------+------------------+\n",
      "|customer_id|       full_name|recency|        avg_amount|\n",
      "+-----------+----------------+-------+------------------+\n",
      "|          1|      MARY SMITH|   3992| 49.03203883495146|\n",
      "|          2|PATRICIA JOHNSON|   3963| 46.43461538461538|\n",
      "|          3|  LINDA WILLIAMS|   3406| 45.36190476190477|\n",
      "|          4|   BARBARA JONES|   3922| 50.99411764705882|\n",
      "|          5| ELIZABETH BROWN|   3889|51.247272727272716|\n",
      "|          6|  JENNIFER DAVIS|   3994| 49.39318181818181|\n",
      "|          7|    MARIA MILLER|   3994|56.424074074074085|\n",
      "|          8|    SUSAN WILSON|   3625| 51.92040816326531|\n",
      "|          9|  MARGARET MOORE|   3776|48.030645161290316|\n",
      "|         10|  DOROTHY TAYLOR|   3899| 50.96379310344826|\n",
      "|         11|   LISA ANDERSON|   3983|  58.5490566037736|\n",
      "|         12|    NANCY THOMAS|   3984|41.300000000000004|\n",
      "|         13|   KAREN JACKSON|   3953| 51.89473684210526|\n",
      "|         14|     BETTY WHITE|   3976| 44.44615384615385|\n",
      "|         15|    HELEN HARRIS|   3977| 49.67450980392158|\n",
      "|         16|   SANDRA MARTIN|   4008| 47.43275862068966|\n",
      "|         17|  DONNA THOMPSON|   3947|45.101724137931036|\n",
      "|         18|    CAROL GARCIA|   4012| 50.51285714285712|\n",
      "|         19|   RUTH MARTINEZ|   3896|53.477586206896554|\n",
      "|         20| SHARON ROBINSON|   3852|52.088235294117666|\n",
      "+-----------+----------------+-------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "recency_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b752cda9-c842-42db-8664-fb721f760aa7",
   "metadata": {},
   "source": [
    "## Save data into MinIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "674bb3a9-c1bf-43f8-946e-3f35273658b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-----------+\n",
      "|namespace|  tableName|isTemporary|\n",
      "+---------+-----------+-----------+\n",
      "|  test_db|    recency|      false|\n",
      "|         |vw_activity|      false|\n",
      "|         |vw_customer|      false|\n",
      "|         | vw_payment|      false|\n",
      "|         |  vw_rental|      false|\n",
      "+---------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"USE test_db\")\n",
    "spark.sql(\"SHOW TABLES\").show()\n",
    "\n",
    "# spark.sql(\"DROP TABLE employee\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c7cf1f-0df8-4ea7-b09b-4fa33e73643e",
   "metadata": {},
   "source": [
    "- **Save dataframe into Parquet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "adaffbfe-38fa-4886-80af-0fc8354afcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "recency_df.write.format(\"parquet\").save(\"s3a://lakehouse/test_parquet/recency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68d6948-2d87-408e-b8d6-a2bcfa09906e",
   "metadata": {},
   "source": [
    "- **Save dataframe into Delta Lake**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "643fa1ae-06bf-4305-98e1-624d09ddae70",
   "metadata": {},
   "outputs": [],
   "source": [
    "recency_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"test_db.recency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a6e689-8713-41d8-8e20-fdcf2ed1150e",
   "metadata": {},
   "source": [
    "- **Check the metadata of table in Hive Metastore**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "82339752-82b4-4df5-a9e4-b111e8dff4be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+\n",
      "|            col_name|           data_type|comment|\n",
      "+--------------------+--------------------+-------+\n",
      "|         customer_id|                 int|       |\n",
      "|           full_name|              string|       |\n",
      "|             recency|                 int|       |\n",
      "|          avg_amount|              double|       |\n",
      "|                    |                    |       |\n",
      "|      # Partitioning|                    |       |\n",
      "|     Not partitioned|                    |       |\n",
      "|                    |                    |       |\n",
      "|# Detailed Table ...|                    |       |\n",
      "|                Name|     test_db.recency|       |\n",
      "|            Location|s3a://lakehouse/t...|       |\n",
      "|            Provider|               delta|       |\n",
      "|               Owner|              jovyan|       |\n",
      "|    Table Properties|[delta.minReaderV...|       |\n",
      "+--------------------+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DESCRIBE EXTENDED recency\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5beeccfe-cb68-42ea-90ea-456003c5900b",
   "metadata": {},
   "source": [
    "## Kill Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4d82fc9-5b61-4422-bb62-90ca3f70341f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
