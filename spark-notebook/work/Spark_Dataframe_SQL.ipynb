{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96027ea9-c511-4436-a66f-4a15f2fd08a2",
   "metadata": {},
   "source": [
    "# Spark Dataframe and Spark SQL\n",
    "\n",
    "* Demo between Spark Dataframe and Spark RDD\n",
    "* Spark Dataframe Operations\n",
    "* Spark SQL Operations\n",
    "* Save dataframe into MinIO\n",
    "    * Write a dataframe by Delta Lake format\n",
    "    * Write a dataframe by Parquet format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6cecccaa-d5c9-422b-b933-928390863363",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import sum, count, max, avg, asc, desc, col, date_format, round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "caed7a48-9e6d-48b1-94cd-af416f419fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName('Ingest checkin table into bronze') \\\n",
    "    .master('spark://spark-master:7077') \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"100\") \\\n",
    "    .config(\"hive.metastore.uris\", \"thrift://hive-metastore:9083\") \\\n",
    "    .config('spark.hadoop.fs.s3a.aws.credentials.provider', 'org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider') \\\n",
    "    .config('spark.sql.warehouse.dir', f's3a://lakehouse/') \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "863b0b47-c72b-4e21-b4ed-69bfc180664d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://f96e811bbabb:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://spark-master:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Ingest checkin table into bronze</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f25e390cdf0>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f2b192-281a-4897-934d-49d25b473087",
   "metadata": {},
   "source": [
    "## Demo between Spark Dataframe and Spark RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5d280a-81cf-4576-b4b6-94a9d7319f3a",
   "metadata": {},
   "source": [
    "### 1. Spark RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d81d0f6-3d3e-4446-aeae-ceacd7898fe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Apache Spark has its architectural foundation in the resilient distributed dataset (RDD), a read-only multiset of data items distributed over a cluster of machines, that is maintained in a fault-tolerant way. The Dataframe API was released as an abstraction on top of the RDD, followed by the Dataset API. In Spark 1.x, the RDD was the primary application programming interface (API), but as of Spark 2.x use of the Dataset API is encouraged even though the RDD API is not deprecated. The RDD technology still underlies the Dataset API.',\n",
       " '',\n",
       " \"Spark and its RDDs were developed in 2012 in response to limitations in the MapReduce cluster computing paradigm, which forces a particular linear dataflow structure on distributed programs: MapReduce programs read input data from disk, map a function across the data, reduce the results of the map, and store reduction results on disk. Spark's RDDs function as a working set for distributed programs that offers a (deliberately) restricted form of distributed shared memory.\"]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_rdd = spark.sparkContext.textFile(\"s3a://lakehouse/test_csv/data.txt\")\n",
    "text_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b79a561d-e62d-43fd-b06c-8fdf11379c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working: 1\n",
      "which: 1\n",
      "were: 1\n",
      "way.: 1\n",
      "was: 2\n",
      "use: 1\n",
      "underlies: 1\n",
      "top: 1\n",
      "to: 1\n",
      "though: 1\n",
      "the: 12\n",
      "that: 2\n",
      "technology: 1\n",
      "structure: 1\n",
      "store: 1\n",
      "still: 1\n",
      "shared: 1\n",
      "set: 1\n",
      "results: 2\n",
      "restricted: 1\n"
     ]
    }
   ],
   "source": [
    "seqFunc = (lambda x, y: x+y)\n",
    "combFunc = (lambda c1, c2: c1+c2)\n",
    "\n",
    "wordcount_rdd = text_rdd.flatMap(lambda line: line.split(' '))\\\n",
    "                        .map(lambda word: (word, 1))\\\n",
    "                        .aggregateByKey(0, seqFunc, combFunc)\\\n",
    "                        .sortByKey(False)\n",
    "\n",
    "for word in wordcount_rdd.take(20):\n",
    "    print(f\"{word[0]}: {word[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a505fba5-f3cf-42e6-b4b8-c462bf99624c",
   "metadata": {},
   "source": [
    "### 2. Spark Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "094d4734-756e-414e-888b-7045891921d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                text|\n",
      "+--------------------+\n",
      "|Apache Spark has ...|\n",
      "|                    |\n",
      "|Spark and its RDD...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_df = spark.read.text(\"s3a://lakehouse/test_csv/data.txt\")\n",
    "columns = ['text']\n",
    "text_df = text_df.toDF(*columns)\n",
    "text_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d8440fc-1e1d-4abf-86d1-b0fc7e793bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|      word|count|\n",
      "+----------+-----+\n",
      "|   working|    1|\n",
      "|     which|    1|\n",
      "|      were|    1|\n",
      "|      way.|    1|\n",
      "|       was|    2|\n",
      "|       use|    1|\n",
      "| underlies|    1|\n",
      "|       top|    1|\n",
      "|        to|    1|\n",
      "|    though|    1|\n",
      "|       the|   12|\n",
      "|      that|    2|\n",
      "|technology|    1|\n",
      "| structure|    1|\n",
      "|     store|    1|\n",
      "|     still|    1|\n",
      "|    shared|    1|\n",
      "|       set|    1|\n",
      "|   results|    2|\n",
      "|restricted|    1|\n",
      "+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wordcount_df = text_df.selectExpr(\"explode(split(text, ' ')) as word\")\\\n",
    "                    .groupBy('word')\\\n",
    "                    .agg(count('word').alias('count'))\\\n",
    "                    .sort(desc('word'))\n",
    "wordcount_df.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eddb431-f622-4ac3-9657-2d61bec6d741",
   "metadata": {},
   "source": [
    "## Spark Dataframe Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370abeb3-bcf9-4778-81b5-936df62a4583",
   "metadata": {},
   "source": [
    "### 1. Loading data from MinIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "673304ff-992c-4e65-88f3-4bc50332351a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows:  599\n",
      "+-----------+--------+----------+---------+--------------------+----------+----------+-------------------+-------------------+------+\n",
      "|customer_id|store_id|first_name|last_name|               email|address_id|activebool|        create_date|        last_update|active|\n",
      "+-----------+--------+----------+---------+--------------------+----------+----------+-------------------+-------------------+------+\n",
      "|          1|       1|      MARY|    SMITH|MARY.SMITH@sakila...|         5|         t|2006-02-14 00:00:00|2006-02-15 09:57:20|     1|\n",
      "|          2|       1|  PATRICIA|  JOHNSON|PATRICIA.JOHNSON@...|         6|         t|2006-02-14 00:00:00|2006-02-15 09:57:20|     1|\n",
      "|          3|       1|     LINDA| WILLIAMS|LINDA.WILLIAMS@sa...|         7|         t|2006-02-14 00:00:00|2006-02-15 09:57:20|     1|\n",
      "|          4|       2|   BARBARA|    JONES|BARBARA.JONES@sak...|         8|         t|2006-02-14 00:00:00|2006-02-15 09:57:20|     1|\n",
      "|          5|       1| ELIZABETH|    BROWN|ELIZABETH.BROWN@s...|         9|         t|2006-02-14 00:00:00|2006-02-15 09:57:20|     1|\n",
      "+-----------+--------+----------+---------+--------------------+----------+----------+-------------------+-------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_df = spark.read.csv(\"s3a://lakehouse/test_csv/customer.csv\", header=True, inferSchema=True, samplingRatio=0.1)\n",
    "print(\"Number of rows: \", customer_df.count())\n",
    "customer_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ca4e70f-2b5a-46fa-9aac-cccafa840506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- store_id: integer (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- address_id: integer (nullable = true)\n",
      " |-- activebool: string (nullable = true)\n",
      " |-- create_date: timestamp (nullable = true)\n",
      " |-- last_update: timestamp (nullable = true)\n",
      " |-- active: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44eb8f0b-2b72-4826-a041-66880c63d757",
   "metadata": {},
   "source": [
    "### 2. Selecting columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e76f5ca1-4483-4269-88d1-199739303a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+----------+\n",
      "|customer_id|               email|address_id|\n",
      "+-----------+--------------------+----------+\n",
      "|          1|MARY.SMITH@sakila...|         5|\n",
      "|          2|PATRICIA.JOHNSON@...|         6|\n",
      "|          3|LINDA.WILLIAMS@sa...|         7|\n",
      "|          4|BARBARA.JONES@sak...|         8|\n",
      "|          5|ELIZABETH.BROWN@s...|         9|\n",
      "|          6|JENNIFER.DAVIS@sa...|        10|\n",
      "|          7|MARIA.MILLER@saki...|        11|\n",
      "|          8|SUSAN.WILSON@saki...|        12|\n",
      "|          9|MARGARET.MOORE@sa...|        13|\n",
      "|         10|DOROTHY.TAYLOR@sa...|        14|\n",
      "|         11|LISA.ANDERSON@sak...|        15|\n",
      "|         12|NANCY.THOMAS@saki...|        16|\n",
      "|         13|KAREN.JACKSON@sak...|        17|\n",
      "|         14|BETTY.WHITE@sakil...|        18|\n",
      "|         15|HELEN.HARRIS@saki...|        19|\n",
      "|         16|SANDRA.MARTIN@sak...|        20|\n",
      "|         17|DONNA.THOMPSON@sa...|        21|\n",
      "|         18|CAROL.GARCIA@saki...|        22|\n",
      "|         19|RUTH.MARTINEZ@sak...|        23|\n",
      "|         20|SHARON.ROBINSON@s...|        24|\n",
      "+-----------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## TODO: selecting columns by select()\n",
    "temp_df = customer_df.select(\"customer_id\", \"email\", \"address_id\")\n",
    "temp_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b59376f-ac38-4660-9daf-fc6ace9dfa01",
   "metadata": {},
   "source": [
    "### 3. Filtering records following conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f943fc3-9792-40c2-88a9-e229c3001e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows:  584\n",
      "+-----------+--------+----------+---------+--------------------+----------+----------+-------------------+-------------------+------+\n",
      "|customer_id|store_id|first_name|last_name|               email|address_id|activebool|        create_date|        last_update|active|\n",
      "+-----------+--------+----------+---------+--------------------+----------+----------+-------------------+-------------------+------+\n",
      "|          1|       1|      MARY|    SMITH|MARY.SMITH@sakila...|         5|         t|2006-02-14 00:00:00|2006-02-15 09:57:20|     1|\n",
      "|          2|       1|  PATRICIA|  JOHNSON|PATRICIA.JOHNSON@...|         6|         t|2006-02-14 00:00:00|2006-02-15 09:57:20|     1|\n",
      "|          3|       1|     LINDA| WILLIAMS|LINDA.WILLIAMS@sa...|         7|         t|2006-02-14 00:00:00|2006-02-15 09:57:20|     1|\n",
      "|          4|       2|   BARBARA|    JONES|BARBARA.JONES@sak...|         8|         t|2006-02-14 00:00:00|2006-02-15 09:57:20|     1|\n",
      "|          5|       1| ELIZABETH|    BROWN|ELIZABETH.BROWN@s...|         9|         t|2006-02-14 00:00:00|2006-02-15 09:57:20|     1|\n",
      "+-----------+--------+----------+---------+--------------------+----------+----------+-------------------+-------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## TODO: filtering records by filter()\n",
    "temp_df = customer_df.filter(customer_df.active==1).orderBy(col(\"address_id\").asc())\n",
    "print(\"Number of rows: \", temp_df.count())\n",
    "temp_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29073bbc-1320-492a-ab3f-3414e64c8dbb",
   "metadata": {},
   "source": [
    "### 4. Grouping records with aggregation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28e35b74-0414-411b-8f2e-318edc3403a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+-----------------------------+\n",
      "|store_id|quantity|recent_activity_customer_date|\n",
      "+--------+--------+-----------------------------+\n",
      "|       2|     273|          2006-02-15 09:57:20|\n",
      "|       1|     326|          2006-02-15 09:57:20|\n",
      "+--------+--------+-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## TODO: Grouping records by using groupBy()\n",
    "temp_df = customer_df.groupBy(\"store_id\")\\\n",
    "                    .agg(count(\"customer_id\").alias(\"quantity\"), max(\"last_update\").alias(\"recent_activity_customer_date\"))\n",
    "temp_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464b0efb-c70a-45bc-aab0-93dfd1272379",
   "metadata": {},
   "source": [
    "### 5. Joining the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8a56b746-7119-4526-89e0-7d31f5f8e0bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows:  36938\n",
      "+---------+-------------------+------------+-----------+-------------------+--------+-------------------+\n",
      "|rental_id|        rental_date|inventory_id|customer_id|        return_date|staff_id|        last_update|\n",
      "+---------+-------------------+------------+-----------+-------------------+--------+-------------------+\n",
      "|   281475|2010-01-01 04:21:21|        1372|        238|2010-01-03 00:00:00|      68|2010-01-01 02:18:47|\n",
      "|   281476|2010-01-01 07:47:23|        3430|        167|2010-01-03 04:21:21|     122|2010-01-01 04:05:03|\n",
      "|   281477|2010-01-01 09:15:59|        1161|        446|2010-01-05 07:47:23|       1|2010-01-01 05:08:26|\n",
      "|   281478|2010-01-01 15:20:16|         743|        565|2010-01-06 09:15:59|     120|2010-01-01 06:03:18|\n",
      "|   281479|2010-01-01 20:41:05|        1919|        573|2010-01-06 15:20:16|      21|2010-01-01 06:20:04|\n",
      "+---------+-------------------+------------+-----------+-------------------+--------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## TODO: Joining dataframe by join()\n",
    "rental_df = spark.read.csv(\"s3a://lakehouse/test_csv/rental.csv\", header=True, inferSchema=True)\n",
    "print(\"Number of rows: \", rental_df.count())\n",
    "rental_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "55eb3932-7120-4e00-b74d-fa50a5fae10e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+--------------------+---------+-------------------+--------+\n",
      "|customer_id|store_id|               email|rental_id|        rental_date|staff_id|\n",
      "+-----------+--------+--------------------+---------+-------------------+--------+\n",
      "|        238|       1|NELLIE.GARRETT@sa...|   281475|2010-01-01 04:21:21|      68|\n",
      "|        167|       2|SALLY.PIERCE@saki...|   281476|2010-01-01 07:47:23|     122|\n",
      "|        446|       2|THEODORE.CULP@sak...|   281477|2010-01-01 09:15:59|       1|\n",
      "|        565|       2|JAIME.NETTLES@sak...|   281478|2010-01-01 15:20:16|     120|\n",
      "|        573|       1|BYRON.BOX@sakilac...|   281479|2010-01-01 20:41:05|      21|\n",
      "+-----------+--------+--------------------+---------+-------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined_df = customer_df.join(other=rental_df, on=\"customer_id\", how=\"inner\")\\\n",
    "                    .select(\"customer_id\", \"store_id\", \"email\", \"rental_id\", \"rental_date\", \"staff_id\")\n",
    "joined_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63df6b57-4b5d-46d5-b8ec-45f546a40e95",
   "metadata": {},
   "source": [
    "### 6. Sorting records by columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2767f537-e608-4c10-9595-5f76763abb16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+------------+-----------+-------------------+--------+-------------------+\n",
      "|rental_id|        rental_date|inventory_id|customer_id|        return_date|staff_id|        last_update|\n",
      "+---------+-------------------+------------+-----------+-------------------+--------+-------------------+\n",
      "|   318412|2020-12-30 00:23:16|        2200|         44|2020-12-30 23:48:02|      76|2015-03-31 13:16:31|\n",
      "|   318411|2020-12-29 23:48:02|        2217|         65|2021-01-03 22:59:50|      11|2015-03-31 11:32:29|\n",
      "|   318410|2020-12-29 22:59:50|        1440|        235|2021-01-02 22:33:28|       7|2015-03-31 10:00:55|\n",
      "|   318409|2020-12-29 22:33:28|        2809|        346|2020-12-31 22:14:03|      64|2015-03-31 09:13:12|\n",
      "|   318408|2020-12-29 22:14:03|        3089|        554|2020-12-31 21:10:26|      78|2015-03-31 08:50:00|\n",
      "+---------+-------------------+------------+-----------+-------------------+--------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sorted_df = rental_df.orderBy(col(\"rental_date\").desc(), col(\"customer_id\").asc())\n",
    "sorted_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc401cb3-e7ed-42fe-b965-2ae28ed7fb51",
   "metadata": {},
   "source": [
    "### 7. Window Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "28b73d0e-a101-4a1c-bd74-58ea9500e0dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+--------+---------+------+-------------------+-------------------+\n",
      "|payment_id|customer_id|staff_id|rental_id|amount|       payment_date|        last_update|\n",
      "+----------+-----------+--------+---------+------+-------------------+-------------------+\n",
      "|     40237|        238|      71|   281475|  39.3|2010-01-03 00:00:00|2010-01-01 00:10:25|\n",
      "|     40238|        167|      80|   281476|  83.7|2010-01-03 04:21:21|2010-01-01 00:21:13|\n",
      "|     40239|        446|     121|   281477|  38.5|2010-01-05 07:47:23|2010-01-01 00:33:58|\n",
      "|     40240|        565|      19|   281478|  14.8|2010-01-06 09:15:59|2010-01-01 00:48:18|\n",
      "|     40241|        573|      64|   281479|  67.0|2010-01-06 15:20:16|2010-01-01 01:03:01|\n",
      "+----------+-----------+--------+---------+------+-------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "payment_df = spark.read.csv(\"s3a://lakehouse/test_csv/payment.csv\", header=True, inferSchema=True)\n",
    "payment_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e1040f-b6f7-4088-a8bd-ce4a8034825e",
   "metadata": {},
   "source": [
    "- **Using Window Function for finding contribution precentage in a mounth**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "48a07d54-6c8a-4184-80aa-3987f1ae58bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+--------+---------+------+-------------------+-------------------+-----------------+\n",
      "|payment_id|customer_id|staff_id|rental_id|amount|       payment_date|        last_update|percent_in_mounth|\n",
      "+----------+-----------+--------+---------+------+-------------------+-------------------+-----------------+\n",
      "|     40396|          1|     113|   281651|  53.4|2010-01-22 08:02:16|2010-01-02 10:55:56|            100.0|\n",
      "|     40707|          1|       3|   281990|  60.3|2010-03-02 14:44:06|2010-01-05 08:34:39|            53.79|\n",
      "|     40806|          1|       2|   282094|   7.6|2010-03-10 16:55:31|2010-01-06 06:56:51|             6.78|\n",
      "|     40843|          1|      12|   282140|  44.2|2010-03-22 08:20:36|2010-01-06 14:47:55|            39.43|\n",
      "|     41432|          1|      78|   282774|  49.3|2010-07-30 06:14:08|2010-01-12 01:37:03|            100.0|\n",
      "+----------+-----------+--------+---------+------+-------------------+-------------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## TODO: definite a Window\n",
    "window_spec = Window.partitionBy(col(\"customer_id\"), date_format(col(\"payment_date\"), \"yyyy-MM\"))\n",
    "\n",
    "## TODO: compute aggregation over the window\n",
    "percent_df = payment_df.withColumn(\n",
    "    \"percent_in_mounth\", round((col(\"amount\")*100/sum(\"amount\").over(window_spec)), 2)\n",
    ").orderBy(\n",
    "    col(\"customer_id\").asc()\n",
    ")\n",
    "\n",
    "percent_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591ca34f-ed8a-4b36-a7b2-ca5d25c36fb0",
   "metadata": {},
   "source": [
    "- **Using Window Function for moving average in 7 days**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3fe554ae-65b7-4b78-afc0-eec1bdac0f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+--------+---------+------+-------------------+-------------------+------+\n",
      "|payment_id|customer_id|staff_id|rental_id|amount|       payment_date|        last_update|avg_m3|\n",
      "+----------+-----------+--------+---------+------+-------------------+-------------------+------+\n",
      "|     40237|        238|      71|   281475|  39.3|2010-01-03 00:00:00|2010-01-01 00:10:25|  39.3|\n",
      "|     40238|        167|      80|   281476|  83.7|2010-01-03 04:21:21|2010-01-01 00:21:13|  61.5|\n",
      "|     40253|         18|      12|   281492|  93.0|2010-01-03 19:38:45|2010-01-01 03:48:42|  72.0|\n",
      "|     40255|        312|      72|   281494|  63.9|2010-01-04 00:13:16|2010-01-01 04:15:37|69.975|\n",
      "|     40258|         16|      16|   281497|  61.8|2010-01-04 04:14:58|2010-01-01 04:53:17| 68.34|\n",
      "+----------+-----------+--------+---------+------+-------------------+-------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## TODO: definite a window\n",
    "window_spec = Window.orderBy(\"payment_date\").rowsBetween(-7, 0)\n",
    "\n",
    "## TODO: compute aggregation over the window\n",
    "avgmove_df = payment_df.withColumn(\n",
    "    \"avg_m3\", avg(\"amount\").over(window_spec)\n",
    ")\n",
    "avgmove_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50fa757-8836-4ac0-92ea-c541f684a2e1",
   "metadata": {},
   "source": [
    "## Spark SQL Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f779e83d-4abc-4c43-afbf-08211a896dcf",
   "metadata": {},
   "source": [
    "### 1. Register dataframe as Temporary table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "703408fa-759d-40f6-81ab-415da91937e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df.createOrReplaceTempView(\"vw_customer\")\n",
    "rental_df.createOrReplaceTempView(\"vw_rental\")\n",
    "payment_df.createOrReplaceTempView(\"vw_payment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "10da852f-33f6-4b94-aa23-207773d24861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|current_database()|\n",
      "+------------------+\n",
      "|           default|\n",
      "+------------------+\n",
      "\n",
      "+---------+-----------+-----------+\n",
      "|namespace|  tableName|isTemporary|\n",
      "+---------+-----------+-----------+\n",
      "|         |vw_customer|      false|\n",
      "|         | vw_payment|      false|\n",
      "|         |  vw_rental|      false|\n",
      "+---------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT current_database()\").show()\n",
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1c26f9a6-9357-4f2d-9aab-6fa2ad02570a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in vw_customer 599\n",
      "Number of rows in vw_payment 33286\n",
      "Number of rows in vw_rental 36938\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of rows in vw_customer\", spark.sql(\"SELECT * FROM vw_customer\").count())\n",
    "print(\"Number of rows in vw_payment\", spark.sql(\"SELECT * FROM vw_payment\").count())\n",
    "print(\"Number of rows in vw_rental\", spark.sql(\"SELECT * FROM vw_rental\").count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8081bd63-ba23-4866-b71a-e3d04aae9d4a",
   "metadata": {},
   "source": [
    "### 2. Manipulate data with SQL script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a539f7-bdaa-4219-b580-3879a21209d4",
   "metadata": {},
   "source": [
    "- **Joining views and creating a new view**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1c542581-6d2a-440c-9c8a-18a36764f812",
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlStr = \"\"\" \n",
    "    SELECT\n",
    "        c.customer_id, c.active, CONCAT(c.first_name,' ', c.last_name) AS full_name, r.rental_id, r.rental_date, p.amount, p.payment_date\n",
    "    FROM vw_customer c\n",
    "    INNER JOIN vw_rental r ON r.customer_id=c.customer_id\n",
    "    INNER JOIN vw_payment p ON p.rental_id=r.rental_id\n",
    "    ORDER BY c.customer_id, rental_date\n",
    "\"\"\"\n",
    "activities_df = spark.sql(sqlStr)\n",
    "activities_df.createOrReplaceTempView(\"vw_activity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a314141f-d338-446e-b829-e788bcf81fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+----------+---------+-------------------+------+-------------------+\n",
      "|customer_id|active| full_name|rental_id|        rental_date|amount|       payment_date|\n",
      "+-----------+------+----------+---------+-------------------+------+-------------------+\n",
      "|          1|     1|MARY SMITH|   281651|2010-01-21 09:37:31|  53.4|2010-01-22 08:02:16|\n",
      "|          1|     1|MARY SMITH|   281990|2010-02-25 20:16:23|  60.3|2010-03-02 14:44:06|\n",
      "|          1|     1|MARY SMITH|   282094|2010-03-07 19:34:22|   7.6|2010-03-10 16:55:31|\n",
      "|          1|     1|MARY SMITH|   282140|2010-03-19 14:24:00|  44.2|2010-03-22 08:20:36|\n",
      "|          1|     1|MARY SMITH|   282774|2010-07-25 09:54:25|  49.3|2010-07-30 06:14:08|\n",
      "+-----------+------+----------+---------+-------------------+------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM vw_activity LIMIT 5\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f4fe49-9436-4bb7-a5a6-fedb6bdb8b4b",
   "metadata": {},
   "source": [
    "- **Window Function in Spark SQL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3c45e775-fb3e-4187-8b6e-00bca90b144b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlStr = \"\"\"\n",
    "    SELECT DISTINCT\n",
    "        customer_id, full_name,\n",
    "        DATEDIFF(latest_activity_date, earliest_activity_date) AS recency,\n",
    "        AVG(amount) OVER (PARTITION BY customer_id) AS avg_amount\n",
    "    FROM (\n",
    "        SELECT\n",
    "            *,\n",
    "            MAX(rental_date) OVER (PARTITION BY customer_id) AS latest_activity_date,\n",
    "            MIN(rental_date) OVER (PARTITION BY customer_id) AS earliest_activity_date\n",
    "        FROM vw_activity ac\n",
    "    )\n",
    "\"\"\"\n",
    "recency_df = spark.sql(sqlStr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4b3513e4-4caa-4f69-b038-a822d192a13a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+-------+------------------+\n",
      "|customer_id|       full_name|recency|        avg_amount|\n",
      "+-----------+----------------+-------+------------------+\n",
      "|          1|      MARY SMITH|   3992| 49.03203883495146|\n",
      "|          2|PATRICIA JOHNSON|   3963| 46.43461538461538|\n",
      "|          3|  LINDA WILLIAMS|   3406| 45.36190476190477|\n",
      "|          4|   BARBARA JONES|   3922| 50.99411764705882|\n",
      "|          5| ELIZABETH BROWN|   3889|51.247272727272716|\n",
      "+-----------+----------------+-------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "recency_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b752cda9-c842-42db-8664-fb721f760aa7",
   "metadata": {},
   "source": [
    "## Save data into MinIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "674bb3a9-c1bf-43f8-946e-3f35273658b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-----------+\n",
      "|namespace|  tableName|isTemporary|\n",
      "+---------+-----------+-----------+\n",
      "|  test_db|    recency|      false|\n",
      "|         |vw_activity|      false|\n",
      "|         |vw_customer|      false|\n",
      "|         | vw_payment|      false|\n",
      "|         |  vw_rental|      false|\n",
      "+---------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"USE test_db\")\n",
    "spark.sql(\"SHOW TABLES\").show()\n",
    "\n",
    "# spark.sql(\"DROP TABLE recency_02\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c7cf1f-0df8-4ea7-b09b-4fa33e73643e",
   "metadata": {},
   "source": [
    "- **Save dataframe into Parquet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "adaffbfe-38fa-4886-80af-0fc8354afcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "recency_df.write.format(\"parquet\").save(\"s3a://lakehouse/test_parquet/recency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68d6948-2d87-408e-b8d6-a2bcfa09906e",
   "metadata": {},
   "source": [
    "- **Save dataframe into Delta Lake**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "643fa1ae-06bf-4305-98e1-624d09ddae70",
   "metadata": {},
   "outputs": [],
   "source": [
    "recency_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"test_db.recency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a6e689-8713-41d8-8e20-fdcf2ed1150e",
   "metadata": {},
   "source": [
    "- **Check the metadata of table in Hive Metastore**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "82339752-82b4-4df5-a9e4-b111e8dff4be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+\n",
      "|            col_name|           data_type|comment|\n",
      "+--------------------+--------------------+-------+\n",
      "|         customer_id|                 int|       |\n",
      "|           full_name|              string|       |\n",
      "|             recency|                 int|       |\n",
      "|          avg_amount|              double|       |\n",
      "|                    |                    |       |\n",
      "|      # Partitioning|                    |       |\n",
      "|     Not partitioned|                    |       |\n",
      "|                    |                    |       |\n",
      "|# Detailed Table ...|                    |       |\n",
      "|                Name|     test_db.recency|       |\n",
      "|            Location|s3a://lakehouse/t...|       |\n",
      "|            Provider|               delta|       |\n",
      "|               Owner|              jovyan|       |\n",
      "|    Table Properties|[delta.minReaderV...|       |\n",
      "+--------------------+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DESCRIBE EXTENDED recency\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7a8a28-389c-48d0-b28a-35380e413468",
   "metadata": {},
   "source": [
    "- **Read dataframe from Delta table in MinIO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a304432b-ceae-4bd8-8fc5-d2106ca241f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+-------+------------------+\n",
      "|customer_id|       full_name|recency|        avg_amount|\n",
      "+-----------+----------------+-------+------------------+\n",
      "|          1|      MARY SMITH|   3992| 49.03203883495146|\n",
      "|          2|PATRICIA JOHNSON|   3963| 46.43461538461538|\n",
      "|          3|  LINDA WILLIAMS|   3406| 45.36190476190477|\n",
      "|          4|   BARBARA JONES|   3922| 50.99411764705882|\n",
      "|          5| ELIZABETH BROWN|   3889|51.247272727272716|\n",
      "+-----------+----------------+-------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "delta_df = spark.read.format(\"delta\").load(\"s3a://lakehouse/test_db.db/recency\")\n",
    "delta_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5beeccfe-cb68-42ea-90ea-456003c5900b",
   "metadata": {},
   "source": [
    "## Kill Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b4d82fc9-5b61-4422-bb62-90ca3f70341f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cca6bb-8d36-45f5-9092-9d892b12d894",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
