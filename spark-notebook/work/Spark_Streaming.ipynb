{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0ef3b507-c65f-41d1-be03-668cdea6c082",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a63856bf-ff58-4fd2-b639-0d9f0e3b6ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName('Read Streaming') \\\n",
    "    .master('spark://spark-master:7077') \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .config(\"hive.metastore.uris\", \"thrift://hive-metastore:9083\") \\\n",
    "    .config('spark.hadoop.fs.s3a.aws.credentials.provider', 'org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider') \\\n",
    "    .config('spark.sql.warehouse.dir', f's3a://lakehouse/') \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "29780bbc-7fda-4998-a208-53b365940379",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://f96e811bbabb:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://spark-master:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Streaming data for word counting</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f3578101cd0>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "26e4d357-fd88-4be8-861d-8737163152a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: create the source for reading stream\n",
    "lines = spark.readStream.format(\"socket\")\\\n",
    "            .option(\"host\", \"160.191.244.13\")\\\n",
    "            .option(\"port\", \"9999\")\\\n",
    "            .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a9519df1-f944-4b9d-af5b-f62d79ad23c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: DataFrane operations\n",
    "words_df = lines.select(split(col(\"value\"), \"\\\\s\").alias(\"word\"))\n",
    "counts_df = words_df.groupBy(\"word\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3668c951-aa0d-4be6-bf53-391beaa3933b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: create checkpoint folder to save metadata of each batch (stream) -> fault tolerance - consistency (read once)\n",
    "checkpoint_path = \"s3a://lakehouse/streaming/test/checkpoint\"\n",
    "\n",
    "## TODO: create the sink for writing to console\n",
    "streamingQuery = counts_df.writeStream\\\n",
    "                        .format(\"console\")\\\n",
    "                        .outputMode(\"complete\")\\\n",
    "                        .trigger(processingTime=\"1 second\")\\\n",
    "                        .option(\"checkpointlocation\", checkpoint_path)\\\n",
    "                        .start()\n",
    "\n",
    "streamingQuery.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e651a6ce-8626-464b-ac26-14a91c943913",
   "metadata": {},
   "source": [
    "## Write streaming data into Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a890d09-03ae-4b7a-b517-17317ffe56c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: create checkpoint folder to save metadata of each batch (stream) -> fault tolerance - consistency (read once)\n",
    "checkpoint_path = \"s3a://lakehouse/streaming/test/checkpoint\"\n",
    "\n",
    "## TODO: create the sink for writing to console\n",
    "streamingQuery = counts_df.writeStream\\\n",
    "                        .format(\"delta\")\\\n",
    "                        .outputMode(\"complete\")\\\n",
    "                        .trigger(processingTime=\"1 second\")\\\n",
    "                        .option(\"path\", \"s3a://lakehouse/streaming/test/wordcount\")\n",
    "                        .option(\"checkpointlocation\", checkpoint_path)\\\n",
    "                        .start()\n",
    "\n",
    "streamingQuery.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364d1e58-5bdf-4ec0-99ef-3b74ca7b1d8c",
   "metadata": {},
   "source": [
    "- **Load streamed data from delta**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b5cef85d-babe-4ebc-8beb-de3dee1cd438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|        word|count|\n",
      "+------------+-----+\n",
      "|[girlfriend]|    1|\n",
      "| [beautiful]|    1|\n",
      "|    [friend]|    1|\n",
      "|      [also]|    1|\n",
      "|       [are]|    1|\n",
      "|       [you]|    2|\n",
      "|        [as]|    1|\n",
      "|        [my]|    2|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "delta_df = spark.read.format(\"delta\").load(\"s3a://lakehouse/streaming/test/wordcount\")\n",
    "delta_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a2ded592-d1ab-4400-a2e0-8c1d3f6fba53",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
