{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "043cf28b-3926-43e9-920c-60fe66b7bb30",
   "metadata": {},
   "source": [
    "# Apache Spark Introduction\n",
    "\n",
    "* Create a Spark Session\n",
    "* Write a dataframe by Delta Lake format\n",
    "* Write a dataframe by Parquet format\n",
    "* Manipulate data with Spark SQL (HiveQL)\n",
    "* Kill a Spark Session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423c2f96-c0d4-4e60-83eb-55b8f21e9cc7",
   "metadata": {},
   "source": [
    "## 1. Create a Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29c0515c-6483-4e7f-ab4a-b189e6233d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab2cce6f-2a6e-4b6d-b82f-d8fe5d0ed620",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName('Ingest checkin table into bronze') \\\n",
    "    .master('spark://spark-master:7077') \\\n",
    "    .config(\"hive.metastore.uris\", \"thrift://hive-metastore:9083\")\\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\\\n",
    "    .config('spark.hadoop.fs.s3a.aws.credentials.provider', 'org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider')\\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\\\n",
    "    .config('spark.sql.warehouse.dir', f's3a://lakehouse/')\\\n",
    "    .enableHiveSupport()\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f0f0b11-8e00-4b82-a6df-41e24175a7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Generate 100 sample data\n",
    "names = [\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Emma\", \"Frank\", \"Grace\", \"Hannah\", \"Isaac\", \"Julia\"]\n",
    "data = [{\"Name\": random.choice(names), \"Age\": random.randint(20, 40)} for _ in range(100)]\n",
    "\n",
    "df = spark.createDataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f6cda8b2-1503-40a8-b2e5-1a19a94a6e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|Age| Name|\n",
      "+---+-----+\n",
      "| 32| Emma|\n",
      "| 28|Isaac|\n",
      "| 40|Julia|\n",
      "| 24|David|\n",
      "| 26|Alice|\n",
      "+---+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d3e8900-ada8-47da-938c-e695527fcf20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Age: long (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "107a5419-ae30-4813-9aad-3918c8f2e926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of rows 100\n",
      "The number of columns 2\n"
     ]
    }
   ],
   "source": [
    "print(\"The number of rows\", df.count())\n",
    "print(\"The number of columns\", len(df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0c90a7-9c54-4510-8fda-12dc5b32e3b8",
   "metadata": {},
   "source": [
    "## 2. Write Dataframe by Delta Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9d33a784-3c5a-4b69-83b5-61f246319df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS test_db\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d6cc21a6-3b7b-4e34-8223-35ede84569f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sql(\"DROP TABLE employee\").show()\n",
    "df.write.format(\"delta\").saveAsTable(\"test_db.employee\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4119fed8-eb0e-4b4f-aa52-5f234727afa3",
   "metadata": {},
   "source": [
    "## 3. Write Dataframe by Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74360fde-5260-4e3e-9077-c5bc2f765267",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.format(\"parquet\").save(\"s3a://lakehouse/test_write/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a41c6963-4f91-4848-b860-b84e13f9a4d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "|Age|   Name|\n",
      "+---+-------+\n",
      "| 30|    Bob|\n",
      "| 22|Charlie|\n",
      "| 25|  Alice|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parquet_df = spark.read.parquet(\"s3a://lakehouse/test_write/\")\n",
    "parquet_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc78445-1ec1-4156-b32c-08773e435618",
   "metadata": {},
   "source": [
    "## Manipulate data with Spark SQL throughout HiveQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a4114ac6-0bde-42f6-b75c-70a6e0a7ee2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "|  test_db|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW DATABASES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "29636e7f-a5a6-4a45-b756-7980b82aa046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"USE test_db\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "638cf867-afcb-48bd-9ede-dc195217d1c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "|  test_db| employee|      false|\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "371d3e82-82e4-48a0-a6c9-d382f70beb48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------+-------+\n",
      "|       col_name|data_type|comment|\n",
      "+---------------+---------+-------+\n",
      "|            Age|   bigint|       |\n",
      "|           Name|   string|       |\n",
      "|               |         |       |\n",
      "| # Partitioning|         |       |\n",
      "|Not partitioned|         |       |\n",
      "+---------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DESCRIBE employee\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "efdd1875-fffe-4006-b55c-ad4003360fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+\n",
      "|Age|count(Age)|\n",
      "+---+----------+\n",
      "| 40|         9|\n",
      "| 39|         5|\n",
      "| 38|         3|\n",
      "| 37|         4|\n",
      "| 35|         4|\n",
      "| 34|         4|\n",
      "| 33|         5|\n",
      "| 32|         6|\n",
      "| 31|         4|\n",
      "| 30|         8|\n",
      "| 29|         5|\n",
      "| 28|         4|\n",
      "| 27|         3|\n",
      "| 26|         6|\n",
      "| 25|         3|\n",
      "| 24|         3|\n",
      "| 23|         8|\n",
      "| 22|         2|\n",
      "| 21|         6|\n",
      "| 20|         8|\n",
      "+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT Age, COUNT(Age) FROM employee GROUP BY Age ORDER BY Age DESC\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e5636c-26ca-4281-9625-018f2cba8620",
   "metadata": {},
   "source": [
    "## 4. Kill Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "408c7059-5adc-4702-8b63-8e33d228df7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
